import random
from collections import defaultdict
from typing import List, Tuple, Dict, Any
import torch
import math

import numpy as np

Skywork_SYSTEM_PROMPT = """You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction.
Select Assistant 1 or Assistant 2, that is better for the given instruction. The two responses are generated by two different AI assistants respectively.
Do NOT say both / neither are good.

Here are some rules of the evaluation:
(1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response.
(3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
- The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are equally likely to be the better.
- The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.
(5) Your output should only consist of '\\boxed{Assistant 1}' if assistant 1 is better, or '\\boxed{Assistant 2}' if assistant 2 is better. Omit any other output.

"""

Skywork_PROMPT = """## Query

{question}

## Assistant responses

### Assistant 1

{answer1}


### Assistant 2

{answer2}

"""

Skywork_ASSISTANT_PROMPT = """## Analysis

Let's analyze this step by step and decide which assistant is better, and then answer \\boxed{Assistant 1} or \\boxed{Assistant 2}."""


def is_reasoning(reward_model):
    if hasattr(reward_model, 'classifier'):
        return True
    elif hasattr(reward_model, 'lm_head'):
        return False
    else:
        raise ValueError(f"{reward_model} is not a recognized model.")

def get_reward(reward_model, reward_tokenizer, prompts, completions, texts, script_args):
    if is_reasoning(reward_model):
        return get_reward_reasoning(reward_model, reward_tokenizer, prompts, completions, texts, script_args)
    else:
        return get_reward_rm(reward_model, reward_tokenizer, prompts, completions, texts, script_args)


def get_reward_rm(reward_model, reward_tokenizer, prompts, completions, texts, script_args):
    if texts is None:
        texts = [p + c for p, c in zip(prompts, completions)]
    reward_inputs = reward_tokenizer(
        text=texts, return_tensors="pt", padding=True, padding_side="left", add_special_tokens=False,
    )
    reward_inputs = prepare_input(reward_inputs, device=reward_model.device)
    with torch.inference_mode():
        reward = reward_model(**reward_inputs).logits[:, 0]  # Shape (B*G,)
    if script_args.reference_rewards:
        raise NotImplementedError("Reference rewards are not implemented yet.")
    if script_args.sigmoid_rewards:
        reward = torch.sigmoid(reward)
    return reward



def _calculate_bradley_terry_scores(
        num_players: int,
        matches: List[Tuple[int, int]],
        iterations: int = 20
) -> np.ndarray:
    """
    Calculates player strengths using the Bradley-Terry model from pairwise results.

    Args:
        num_players (int): The number of players (completions) in the tournament.
        matches (list): A list of tuples (winner_index, loser_index).
        iterations (int): The number of iterations for the algorithm.

    Returns:
        np.ndarray: An array of reward scores, one for each player.
    """
    if num_players <= 1 or not matches:
        return np.zeros(num_players)

    wins = np.zeros(num_players)
    matchups = defaultdict(list)
    for winner, loser in matches:
        wins[winner] += 1
        matchups[winner].append(loser)
        matchups[loser].append(winner)

    # Initialize scores (strengths)
    p = np.ones(num_players)

    # Iteratively update scores
    for _ in range(iterations):
        p_new = np.copy(p)
        for i in range(num_players):
            if wins[i] > 0:
                denominator = sum(1 / (p[i] + p[j]) for j in matchups[i])
                if denominator > 1e-9:  # Avoid division by zero
                    p_new[i] = wins[i] / denominator
        p = p_new

    # Use the logarithm of the scores as the final reward.
    # Add a small epsilon for stability if a score is zero.
    log_scores = np.log(p + 1e-9)

    # Normalize scores to have a mean of 0 for stability in RL training
    log_scores -= np.mean(log_scores)
    return log_scores


def _run_batched_pairwise_comparisons(
        reward_model,
        reward_tokenizer,
        comparison_requests: List[Dict[str, Any]],
        max_length: int = 4096,
        max_new_tokens: int = 8192,
        generation_batch_size: int = 16,
) -> List[float]:
    """
    Runs all pairwise comparisons for a given round in a batched manner.
    This function internally handles random swapping of completions to mitigate
    positional bias and returns the corrected preference scores.
    """
    if not comparison_requests:
        return []

    all_preferences = []

    for i in range(0, len(comparison_requests), generation_batch_size):
        batch_requests = comparison_requests[i:i + generation_batch_size]
        prompts_for_model = []
        swaps_in_batch = []

        for req in batch_requests:
            comp1, comp2 = req['comp1'], req['comp2']

            swap = random.random() > 0.5
            swaps_in_batch.append(swap)
            if swap:
                comp1, comp2 = comp2, comp1

            user_prompt = Skywork_PROMPT.format(question=req['prompt'], answer1=comp1, answer2=comp2)
            messages = [
                {"role": "system", "content": Skywork_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ]
            input_text = reward_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            prompts_for_model.append(input_text)

        inputs = reward_tokenizer(
            prompts_for_model,
            return_tensors="pt",
            padding='longest',
            truncation=True,
            max_length=max_length,
            padding_side="left",
        ).to(reward_model.device)

        generation_args = {
            "max_new_tokens": max_new_tokens,
            "temperature": 0.6,
            "do_sample": True,
            "eos_token_id": reward_tokenizer.eos_token_id,
            "pad_token_id": reward_tokenizer.pad_token_id,
        }

        with torch.no_grad():
            outputs = reward_model.generate(**inputs, **generation_args)

        input_token_len = inputs['input_ids'].shape[1]
        generated_tokens = outputs[:, input_token_len:]
        generated_texts = reward_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        print("Generated texts:")

        for j, text in enumerate(generated_texts):
            preference = extract_reward_from_response(text)
            print(text, preference)
            print("\n\n\n\n")
            # Un-swap the result if we swapped the inputs initially
            if swaps_in_batch[j]:
                preference *= -1
            all_preferences.append(preference)

    return all_preferences


def get_reward_reasoning(
        reward_model,
        reward_tokenizer,
        prompts: List[str],
        completions: List[str],
        generation_batch_size: int = 16,
) -> torch.Tensor:
    """
    Computes rewards for completions using a Swiss-system tournament.

    This function groups completions by prompt, runs a tournament for each group,
    and calculates final rewards using a Bradley-Terry model. Inference is batched
    per tournament round across all active tournaments for maximum efficiency.
    """
    if len(prompts) != len(completions):
        raise ValueError("Length of prompts and completions must be equal.")

    grouped_completions = defaultdict(list)
    for i, (p, c) in enumerate(zip(prompts, completions)):
        grouped_completions[p].append({"completion": c, "original_idx": i})

    tournaments_data = []
    max_num_players = 0
    for prompt, group in grouped_completions.items():
        num_players = len(group)
        if num_players > max_num_players:
            max_num_players = num_players

        tournaments_data.append({
            'prompt': prompt,
            'group_info': group,
            'num_players': num_players,
            'players_state': [{'id': i, 'score': 0, 'opponents': set()} for i in range(num_players)],
            'all_matches': [],
        })

    if max_num_players <= 1:
        return torch.zeros(len(completions), dtype=torch.float32, device=reward_model.device)

    num_total_rounds = math.ceil(math.log2(max_num_players))

    for round_num in range(num_total_rounds):
        requests_this_round = []

        for tournament_idx, tournament in enumerate(tournaments_data):
            if tournament['num_players'] <= 1:
                continue

            tournament['players_state'].sort(key=lambda x: x['score'], reverse=True)
            paired_in_round = [False] * tournament['num_players']
            players_state = tournament['players_state']

            for i in range(tournament['num_players']):
                if paired_in_round[i]:
                    continue

                for j in range(i + 1, tournament['num_players']):
                    if not paired_in_round[j] and players_state[j]['id'] not in players_state[i]['opponents']:
                        player1_state, player2_state = players_state[i], players_state[j]

                        # The request no longer needs to track swapping
                        requests_this_round.append({
                            "prompt": tournament['prompt'],
                            "comp1": tournament['group_info'][player1_state['id']]['completion'],
                            "comp2": tournament['group_info'][player2_state['id']]['completion'],
                            "tournament_idx": tournament_idx,
                            "player1_state_idx": i,
                            "player2_state_idx": j,
                        })

                        paired_in_round[i], paired_in_round[j] = True, True
                        break

        if not requests_this_round:
            break

        preferences = _run_batched_pairwise_comparisons(
            reward_model, reward_tokenizer, requests_this_round, generation_batch_size=generation_batch_size
        )

        for i, request in enumerate(requests_this_round):
            preference = preferences[i]
            tourn_idx = request['tournament_idx']
            p1_state_idx, p2_state_idx = request['player1_state_idx'], request['player2_state_idx']

            player1_state = tournaments_data[tourn_idx]['players_state'][p1_state_idx]
            player2_state = tournaments_data[tourn_idx]['players_state'][p2_state_idx]
            p1_id, p2_id = player1_state['id'], player2_state['id']

            player1_state['opponents'].add(p2_id)
            player2_state['opponents'].add(p1_id)

            if preference > 0:
                player1_state['score'] += 1
                tournaments_data[tourn_idx]['all_matches'].append((p1_id, p2_id))
            elif preference < 0:
                player2_state['score'] += 1
                tournaments_data[tourn_idx]['all_matches'].append((p2_id, p1_id))

    final_rewards = torch.zeros(len(completions), dtype=torch.float32)
    for tournament in tournaments_data:
        group_rewards = _calculate_bradley_terry_scores(
            tournament['num_players'], tournament['all_matches']
        )
        for i in range(tournament['num_players']):
            original_idx = tournament['group_info'][i]['original_idx']
            final_rewards[original_idx] = group_rewards[i]

    print("Tournaments data:")
    print(tournaments_data)
    print("Final rewards:")
    print(final_rewards)

    return final_rewards.to(reward_model.device)


def extract_reward_from_response(response):
    lower_response = response.lower()
    pos_assistant1 = lower_response.rfind("assistant 1")
    pos_assistant2 = lower_response.rfind("assistant 2")

    # If neither phrase is found, rfind() returns -1 for both.
    if pos_assistant1 == -1 and pos_assistant2 == -1:
        return 0

    # The one with the greater index appeared later in the text and is the likely choice.
    # This comparison works even if one of them is -1.
    if pos_assistant1 > pos_assistant2:
        return 1
    else:
        return -1